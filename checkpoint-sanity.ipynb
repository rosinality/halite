{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44518fef-d9e5-4ad8-a75a-3872b55cccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea03ed41-6e67-4b89-b1e3-5e074b9fce03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/naplm/seonghyeon\n"
     ]
    }
   ],
   "source": [
    "cd /mnt/naplm/seonghyeon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa6699f-94ad-4686-a000-5d5f295085c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from slickconf import load_config, instantiate\n",
    "import torch.distributed.checkpoint as dcp\n",
    "import json\n",
    "import torch\n",
    "from infer.llama import Transformer\n",
    "from infer.model_args import ModelArgs\n",
    "from halite.utils.probe import Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee1a5773-9b21-405d-80b2-28f88df40d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = load_config('halite/configs/models/llama/llama3_2_3b.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42262e20-217d-4927-9a2d-0dabc7db6ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = instantiate(conf.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e17dcc2b-467d-4fa9-bfce-99e9ad583322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsml/.local/lib/python3.11/site-packages/torch/distributed/checkpoint/state_dict_loader.py:142: UserWarning: torch.distributed is unavailable or uninitialized, assuming the intent is to load in a single process.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "state_dict = {\"model\": model.state_dict()}\n",
    "dcp.load(\n",
    "    state_dict=state_dict,\n",
    "    checkpoint_id='/mnt/naplm/seonghyeon/llama/halite/llama3.2-3b/',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d43da40-b2ab-4f11-bf0b-86c5bd38c837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('llama/checkpoints/Llama3.2-3B/params.json', \"r\") as f:\n",
    "    params = json.loads(f.read())\n",
    "model_args: ModelArgs = ModelArgs(\n",
    "    max_seq_len=8192,\n",
    "    max_batch_size=16,\n",
    "    **params,\n",
    ")\n",
    "checkpoint = torch.load('llama/checkpoints/Llama3.2-3B/consolidated.00.pth', map_location=\"cpu\", weights_only=True)\n",
    "model_orig = Transformer(model_args)\n",
    "model_orig.load_state_dict(checkpoint, strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73fed3c5-5457-4fa1-aae6-8de3b08c3299",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_orig = model_orig.to('cuda').to(torch.bfloat16).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1e02957d-5f8a-4a11-b3bd-5aa9dd931b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to('cuda').to(torch.bfloat16).eval()\n",
    "model_orig = model_orig.to('cuda').to(torch.bfloat16).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3238d970-b178-4fab-92fc-5ed423538118",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8]], device='cuda')\n",
    "start_pos = 0\n",
    "position_ids = torch.tensor([[0, 1, 2, 3, 4, 5, 6, 7]], device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "922e1015-6f46-424c-a413-9d4d169aaab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe = Probe('llama-debug/halite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6857c04-6928-41de-a829-277a91700758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0356,  0.0009, -0.0229,  ..., -0.0317,  0.0359, -0.0065],\n",
      "         [ 0.0009,  0.0068, -0.0610,  ..., -0.0201,  0.0206,  0.0133],\n",
      "         [ 0.0045,  0.0459, -0.0669,  ...,  0.0486,  0.0190, -0.0203],\n",
      "         ...,\n",
      "         [ 0.0278,  0.0013, -0.0620,  ...,  0.0254,  0.0493,  0.0156],\n",
      "         [-0.0014, -0.0036, -0.0376,  ...,  0.0032,  0.0173,  0.0238],\n",
      "         [-0.0283,  0.0019, -0.0581,  ..., -0.0183, -0.0032,  0.0242]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[ 0.0118,  0.0088, -0.1660,  ..., -0.0583, -0.0361, -0.0391],\n",
      "         [ 0.0140, -0.0047, -0.1533,  ..., -0.0889, -0.0232, -0.0126],\n",
      "         [ 0.0315, -0.0065, -0.1660,  ...,  0.0160, -0.0283, -0.0781],\n",
      "         ...,\n",
      "         [ 0.0144, -0.0742, -0.1426,  ..., -0.0057,  0.0354,  0.0454],\n",
      "         [ 0.0068, -0.0430, -0.0654,  ..., -0.0554, -0.0073,  0.0330],\n",
      "         [-0.0312, -0.0297, -0.0264,  ..., -0.0015, -0.0371,  0.0188]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.0532,  0.2539,  2.1250,  ...,  2.5000,  0.2773,  0.2773],\n",
      "         [ 0.0298, -0.0177, -0.2129,  ...,  0.0081,  0.0184, -0.0287],\n",
      "         [ 0.0752, -0.0195, -0.1787,  ...,  0.0586, -0.0093, -0.0645],\n",
      "         ...,\n",
      "         [ 0.0938, -0.1162, -0.1904,  ...,  0.0251,  0.0762,  0.1260],\n",
      "         [ 0.0193, -0.0815, -0.0544,  ...,  0.0293,  0.0654,  0.0208],\n",
      "         [-0.0080, -0.0094, -0.0481,  ..., -0.0112, -0.0171,  0.0100]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.0583,  0.2949,  2.1719,  ...,  2.5469,  0.3223,  0.2539],\n",
      "         [ 0.0190, -0.0530, -0.3223,  ...,  0.0513, -0.0386, -0.0938],\n",
      "         [ 0.0581, -0.0791, -0.2578,  ...,  0.0332, -0.0864, -0.0684],\n",
      "         ...,\n",
      "         [ 0.1157, -0.1465, -0.3125,  ...,  0.0498,  0.0933, -0.0112],\n",
      "         [ 0.0889, -0.1108, -0.0791,  ...,  0.0967,  0.1045, -0.0148],\n",
      "         [-0.0518,  0.0282, -0.1035,  ...,  0.0435, -0.0540,  0.0232]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-4.6631e-02,  3.5352e-01,  2.2031e+00,  ...,  2.5625e+00,\n",
      "           2.6562e-01,  2.6562e-01],\n",
      "         [ 3.1494e-02, -1.5918e-01, -3.1836e-01,  ...,  6.9336e-02,\n",
      "          -1.1621e-01, -1.2012e-01],\n",
      "         [ 1.7090e-02, -1.3477e-01, -3.1250e-01,  ...,  6.2988e-02,\n",
      "          -1.7285e-01, -3.6621e-03],\n",
      "         ...,\n",
      "         [ 9.0820e-02, -9.1797e-02, -3.2617e-01,  ...,  6.7871e-02,\n",
      "          -3.7109e-02,  8.9111e-03],\n",
      "         [ 8.1543e-02, -1.0645e-01, -6.8359e-02,  ...,  7.5684e-02,\n",
      "           1.0498e-02,  4.2480e-02],\n",
      "         [-6.0791e-02, -1.3184e-02, -1.4062e-01,  ..., -1.2207e-03,\n",
      "          -1.1230e-02,  1.2207e-01]]], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.0742,  0.3906,  2.2812,  ...,  2.5312,  0.3320,  0.3223],\n",
      "         [ 0.0186, -0.0244, -0.3086,  ...,  0.0986, -0.0811, -0.0693],\n",
      "         [ 0.0752, -0.1426, -0.3164,  ...,  0.0303, -0.0830, -0.0082],\n",
      "         ...,\n",
      "         [ 0.1426, -0.1973, -0.2773,  ..., -0.1846, -0.1455, -0.0427],\n",
      "         [ 0.0349, -0.2080,  0.0129,  ..., -0.0317, -0.1138,  0.0164],\n",
      "         [-0.0231, -0.0728, -0.1328,  ..., -0.0664, -0.0981,  0.0286]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-8.8867e-02,  4.8047e-01,  2.3125e+00,  ...,  2.5938e+00,\n",
      "           3.3008e-01,  2.8711e-01],\n",
      "         [-4.9316e-02,  2.3560e-02, -2.8906e-01,  ...,  1.0596e-01,\n",
      "           1.7090e-02, -9.5215e-03],\n",
      "         [ 1.0254e-01, -1.7212e-02, -2.5586e-01,  ...,  5.0537e-02,\n",
      "          -6.9824e-02,  3.5645e-02],\n",
      "         ...,\n",
      "         [ 8.0566e-03, -1.3867e-01, -1.7871e-01,  ..., -7.9102e-02,\n",
      "          -1.7676e-01,  4.9561e-02],\n",
      "         [-2.9541e-02, -1.2354e-01,  9.1797e-02,  ..., -2.6367e-02,\n",
      "          -1.9043e-01,  7.9102e-02],\n",
      "         [-9.0820e-02, -4.6631e-02, -1.8799e-02,  ...,  2.0020e-02,\n",
      "          -1.6797e-01,  1.4648e-03]]], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.0688,  0.5352,  2.3281,  ...,  2.5000,  0.3184,  0.3516],\n",
      "         [-0.1182,  0.0571, -0.2578,  ...,  0.1348,  0.0967,  0.0052],\n",
      "         [ 0.0352, -0.0127, -0.2471,  ...,  0.1230, -0.0513,  0.0115],\n",
      "         ...,\n",
      "         [-0.0645,  0.0635, -0.1992,  ..., -0.0128,  0.0942, -0.0120],\n",
      "         [-0.1328,  0.0151,  0.0684,  ..., -0.0396, -0.0144, -0.0132],\n",
      "         [-0.0742,  0.1523,  0.0171,  ...,  0.0040, -0.1006, -0.0732]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.1250,  0.6094,  2.2969,  ...,  2.3750,  0.2812,  0.3516],\n",
      "         [-0.0874,  0.0308, -0.2168,  ..., -0.0549,  0.0461, -0.0214],\n",
      "         [ 0.0371, -0.1309, -0.1924,  ...,  0.1309, -0.0332, -0.0266],\n",
      "         ...,\n",
      "         [ 0.0374, -0.1191, -0.2061,  ...,  0.0299,  0.0752, -0.0056],\n",
      "         [-0.0298, -0.0688,  0.0640,  ...,  0.0115,  0.0342, -0.0830],\n",
      "         [ 0.0464,  0.1377,  0.0071,  ..., -0.0024, -0.0674, -0.0297]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-1.4258e-01,  6.0938e-01,  2.2812e+00,  ...,  2.3281e+00,\n",
      "           3.0078e-01,  4.0820e-01],\n",
      "         [-2.6367e-02,  1.0254e-02, -2.0215e-01,  ..., -1.3672e-01,\n",
      "          -9.7656e-04, -2.3804e-02],\n",
      "         [ 7.9102e-02, -9.7656e-02, -1.7773e-01,  ...,  1.0986e-01,\n",
      "          -5.1025e-02, -8.8867e-02],\n",
      "         ...,\n",
      "         [-1.0449e-01, -1.6211e-01, -1.5723e-01,  ...,  7.6172e-02,\n",
      "           9.4727e-02, -1.0938e-01],\n",
      "         [-2.0117e-01, -1.2256e-01,  8.4473e-02,  ...,  2.0752e-02,\n",
      "           1.8188e-02, -9.7168e-02],\n",
      "         [-1.1963e-02,  6.8359e-02,  2.1729e-02,  ...,  1.8311e-02,\n",
      "          -3.8574e-02, -6.3477e-02]]], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.0610,  0.6289,  2.2656,  ...,  2.2344,  0.3398,  0.3633],\n",
      "         [ 0.0168,  0.0437, -0.1934,  ..., -0.1011,  0.0151, -0.0884],\n",
      "         [ 0.1777, -0.1260, -0.1689,  ...,  0.1484, -0.0579, -0.0850],\n",
      "         ...,\n",
      "         [-0.0605, -0.1670, -0.1187,  ...,  0.2354,  0.2168, -0.0493],\n",
      "         [-0.2256, -0.2188,  0.0947,  ...,  0.0155, -0.0116,  0.0374],\n",
      "         [-0.0986, -0.0840, -0.0123,  ...,  0.0898, -0.0605,  0.0122]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.0923,  0.6758,  2.2344,  ...,  2.0938,  0.2969,  0.4180],\n",
      "         [-0.1318, -0.0383, -0.1973,  ..., -0.0605,  0.1172,  0.0048],\n",
      "         [ 0.0266, -0.1055, -0.2129,  ...,  0.0767, -0.0298, -0.0674],\n",
      "         ...,\n",
      "         [-0.1191, -0.1699, -0.1104,  ...,  0.1699,  0.1069, -0.0415],\n",
      "         [-0.2500, -0.2109,  0.0908,  ..., -0.0527,  0.0154,  0.1406],\n",
      "         [-0.2266, -0.1445, -0.0165,  ..., -0.0195, -0.0688,  0.0269]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.1562,  0.7188,  2.1562,  ...,  1.8594,  0.3320,  0.3730],\n",
      "         [-0.1758, -0.1182, -0.2949,  ..., -0.1035,  0.0991,  0.0327],\n",
      "         [ 0.0649, -0.1934, -0.2773,  ..., -0.0125, -0.0464, -0.0186],\n",
      "         ...,\n",
      "         [-0.0962, -0.2949, -0.1680,  ..., -0.0051,  0.0059, -0.0918],\n",
      "         [-0.1973, -0.3359,  0.0703,  ..., -0.0942, -0.0698,  0.0383],\n",
      "         [-0.1670, -0.3145, -0.1299,  ..., -0.1123, -0.0190,  0.0203]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-1.3867e-01,  6.2500e-01,  2.0156e+00,  ...,  1.6406e+00,\n",
      "           3.1250e-01,  3.7695e-01],\n",
      "         [-9.8145e-02, -1.4746e-01, -3.2617e-01,  ..., -1.7090e-01,\n",
      "           7.2754e-02,  5.8594e-02],\n",
      "         [ 8.2520e-02, -1.6699e-01, -2.2070e-01,  ...,  1.8066e-02,\n",
      "          -4.5166e-02,  6.4453e-02],\n",
      "         ...,\n",
      "         [-9.7168e-02, -2.7930e-01, -1.7285e-01,  ...,  1.8555e-02,\n",
      "          -4.9316e-02, -8.9355e-02],\n",
      "         [-1.8359e-01, -2.5000e-01,  1.7383e-01,  ..., -7.7637e-02,\n",
      "          -7.1777e-02,  4.0283e-03],\n",
      "         [-1.1914e-01, -3.3203e-01, -1.6016e-01,  ..., -8.4961e-02,\n",
      "           1.8311e-04, -9.7046e-03]]], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.0850,  0.4043,  1.6328,  ...,  1.4219,  0.4570,  0.3594],\n",
      "         [ 0.1030, -0.1738, -0.3770,  ..., -0.0938,  0.0608,  0.0222],\n",
      "         [ 0.0503, -0.1206, -0.2539,  ...,  0.0718,  0.0054,  0.0928],\n",
      "         ...,\n",
      "         [-0.0757, -0.2559, -0.2793,  ...,  0.0176, -0.0112, -0.1338],\n",
      "         [-0.1377, -0.2129,  0.1865,  ..., -0.0228, -0.0371, -0.0874],\n",
      "         [-0.1191, -0.2012, -0.1836,  ..., -0.0535,  0.0479,  0.0212]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-1.8945e-01,  3.4570e-01,  1.5391e+00,  ...,  1.3047e+00,\n",
      "           5.9766e-01,  3.7109e-01],\n",
      "         [ 2.5977e-01, -2.8516e-01, -2.9688e-01,  ..., -7.7637e-02,\n",
      "           2.4316e-01,  3.5645e-02],\n",
      "         [ 1.3574e-01, -1.0596e-01, -2.2949e-01,  ...,  1.4258e-01,\n",
      "          -5.4688e-02, -2.2461e-02],\n",
      "         ...,\n",
      "         [-6.8848e-02, -2.3730e-01, -2.8320e-01,  ...,  1.3672e-02,\n",
      "           4.2236e-02, -1.5918e-01],\n",
      "         [ 4.8828e-04, -2.3633e-01,  2.3535e-01,  ..., -4.1992e-02,\n",
      "          -7.7148e-02, -1.6309e-01],\n",
      "         [ 1.5137e-02, -1.5039e-01, -1.6211e-01,  ...,  1.5137e-02,\n",
      "          -1.1523e-01, -5.2490e-02]]], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.1895,  0.2676,  1.4844,  ...,  1.2812,  0.5977,  0.3145],\n",
      "         [ 0.1504, -0.4395, -0.4004,  ..., -0.0220,  0.1406,  0.0376],\n",
      "         [ 0.0508, -0.0217, -0.3535,  ...,  0.1133,  0.0576,  0.0420],\n",
      "         ...,\n",
      "         [-0.0610, -0.2598, -0.3750,  ..., -0.0708, -0.0024, -0.2461],\n",
      "         [-0.0280, -0.2168,  0.1992,  ...,  0.0469, -0.0400, -0.2217],\n",
      "         [-0.0986, -0.2451, -0.2637,  ...,  0.0742,  0.0635, -0.2305]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.2051,  0.2080,  1.4062,  ...,  1.2578,  0.5156,  0.3711],\n",
      "         [ 0.3398, -0.2715, -0.4238,  ...,  0.1885,  0.1758,  0.1299],\n",
      "         [ 0.2871, -0.1069, -0.2754,  ...,  0.2051,  0.1680, -0.0977],\n",
      "         ...,\n",
      "         [-0.0583, -0.2139, -0.3418,  ...,  0.0708,  0.2793, -0.0728],\n",
      "         [ 0.0425, -0.2012,  0.2441,  ...,  0.0688,  0.0854, -0.1201],\n",
      "         [-0.0417, -0.2832, -0.1670,  ...,  0.1157,  0.2734, -0.1396]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.2217,  0.1650,  1.3750,  ...,  1.2422,  0.5000,  0.4434],\n",
      "         [ 0.4766, -0.3945, -0.4805,  ...,  0.1660,  0.2119,  0.2148],\n",
      "         [ 0.3926, -0.1709, -0.0479,  ...,  0.0242,  0.1582, -0.0138],\n",
      "         ...,\n",
      "         [ 0.0249, -0.2617, -0.2324,  ...,  0.0781,  0.1436,  0.0181],\n",
      "         [ 0.1973, -0.2930,  0.3223,  ...,  0.2402, -0.1157, -0.0483],\n",
      "         [ 0.0190, -0.1484, -0.1445,  ...,  0.1514, -0.0215, -0.0146]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.2266,  0.0986,  1.2969,  ...,  1.2500,  0.4648,  0.4766],\n",
      "         [ 0.5781, -0.3340, -0.5547,  ...,  0.1533,  0.1855,  0.3848],\n",
      "         [ 0.5625, -0.1582, -0.1680,  ...,  0.0605,  0.1719,  0.0576],\n",
      "         ...,\n",
      "         [ 0.0048, -0.3164, -0.4102,  ...,  0.1895,  0.4082,  0.0981],\n",
      "         [ 0.3652, -0.3672,  0.2461,  ...,  0.2480,  0.0337, -0.1367],\n",
      "         [ 0.2070, -0.1680, -0.2734,  ...,  0.1807,  0.2217, -0.2002]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.2207,  0.0219,  1.2734,  ...,  1.2188,  0.4707,  0.5078],\n",
      "         [ 0.5898, -0.4863, -0.4766,  ...,  0.2910,  0.2461,  0.4902],\n",
      "         [ 0.6250, -0.1445, -0.1631,  ..., -0.0374,  0.0674,  0.0608],\n",
      "         ...,\n",
      "         [-0.0464, -0.2080, -0.3223,  ...,  0.1162,  0.3223,  0.3125],\n",
      "         [ 0.1611, -0.3945,  0.5000,  ...,  0.2480, -0.0957,  0.1162],\n",
      "         [ 0.0903, -0.1992, -0.0547,  ...,  0.1680,  0.1602,  0.1572]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.1846, -0.0967,  1.2188,  ...,  1.2031,  0.4414,  0.5039],\n",
      "         [ 0.5234, -0.4375, -0.3887,  ...,  0.1455,  0.1875,  0.5078],\n",
      "         [ 0.6641, -0.0413, -0.0889,  ...,  0.1953, -0.0188,  0.0312],\n",
      "         ...,\n",
      "         [ 0.0513, -0.1904, -0.3027,  ...,  0.1777,  0.4336,  0.3730],\n",
      "         [ 0.2383, -0.2559,  0.5820,  ...,  0.2256, -0.0146,  0.1836],\n",
      "         [ 0.1904, -0.1143, -0.0913,  ...,  0.1807,  0.3672,  0.1035]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-1.9141e-01, -2.0508e-01,  1.1172e+00,  ...,  1.1875e+00,\n",
      "           4.0430e-01,  5.6250e-01],\n",
      "         [ 5.0781e-01, -2.3828e-01, -6.1328e-01,  ...,  1.6797e-01,\n",
      "           4.2969e-01,  4.8047e-01],\n",
      "         [ 6.8359e-01, -8.3008e-03, -2.0703e-01,  ...,  1.7773e-01,\n",
      "           2.1875e-01,  8.0566e-02],\n",
      "         ...,\n",
      "         [-5.0781e-02, -2.8125e-01, -3.5742e-01,  ...,  2.2461e-02,\n",
      "           4.9219e-01,  4.8242e-01],\n",
      "         [ 2.8516e-01, -1.8750e-01,  6.5234e-01,  ...,  1.1377e-01,\n",
      "           8.2031e-02,  3.0078e-01],\n",
      "         [ 1.0742e-02, -2.1289e-01, -1.5625e-01,  ...,  6.8665e-04,\n",
      "           3.9844e-01,  1.2500e-01]]], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-1.0059e-01, -3.2031e-01,  1.1094e+00,  ...,  1.1328e+00,\n",
      "           3.6328e-01,  6.1719e-01],\n",
      "         [ 5.1562e-01, -2.3633e-01, -6.7188e-01,  ...,  2.5000e-01,\n",
      "           3.4375e-01,  7.0312e-01],\n",
      "         [ 5.8594e-01, -4.1992e-02, -2.1582e-01,  ...,  2.6172e-01,\n",
      "           9.1797e-02,  1.1572e-01],\n",
      "         ...,\n",
      "         [-7.8613e-02,  3.2812e-01, -3.0469e-01,  ..., -4.8828e-04,\n",
      "           6.2500e-01,  9.2188e-01],\n",
      "         [ 4.2773e-01, -1.5625e-02,  8.2422e-01,  ..., -1.3184e-01,\n",
      "           3.3789e-01,  6.1719e-01],\n",
      "         [ 1.6602e-01,  4.0039e-02, -2.4805e-01,  ..., -3.0273e-02,\n",
      "           5.8203e-01,  7.3438e-01]]], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.0967, -0.4082,  1.0859,  ...,  1.0391,  0.3340,  0.6719],\n",
      "         [ 0.2812, -0.1895, -0.5469,  ...,  0.3379,  0.5586,  0.7891],\n",
      "         [ 0.3711,  0.0625, -0.1221,  ...,  0.5820,  0.2090,  0.1377],\n",
      "         ...,\n",
      "         [-0.1216,  0.5391, -0.3125,  ...,  0.0762,  0.6797,  0.9727],\n",
      "         [ 0.5469,  0.0405,  0.7266,  ...,  0.0459,  0.6758,  0.7773],\n",
      "         [ 0.1924,  0.1357, -0.3945,  ...,  0.0356,  0.6055,  0.8672]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[ 0.0222, -0.6094,  0.8945,  ...,  0.9297,  0.2832,  0.8906],\n",
      "         [ 0.5430, -0.3340, -0.3438,  ...,  0.3691,  0.4375,  0.7344],\n",
      "         [ 0.6719,  0.0476, -0.0186,  ...,  0.6367,  0.0332,  0.2051],\n",
      "         ...,\n",
      "         [-0.3398,  0.7734, -0.5547,  ...,  0.1113,  0.5547,  0.9141],\n",
      "         [ 0.7578,  0.0815,  0.5586,  ...,  0.4336,  0.5156,  1.2109],\n",
      "         [-0.0654, -0.2539, -0.5781,  ...,  0.0347,  0.7773,  1.5078]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.6367, -1.0859,  1.1875,  ...,  1.9844, -0.0270,  1.3594],\n",
      "         [ 0.8086, -0.3516, -0.0884,  ...,  0.3320,  0.6328,  0.2871],\n",
      "         [ 0.9844, -0.0437,  0.3711,  ...,  0.3047,  0.0542,  0.2969],\n",
      "         ...,\n",
      "         [-0.5625,  0.7266, -0.2441,  ..., -0.3418,  0.4590,  1.3438],\n",
      "         [ 0.3105, -0.2422,  0.5430,  ...,  0.3711,  0.8281,  1.1016],\n",
      "         [ 0.0405, -0.2949, -0.3711,  ...,  0.1406,  0.6328,  1.5000]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-1.0156, -0.9180,  1.1406,  ...,  2.8281, -0.5391,  0.5703],\n",
      "         [ 1.0625, -0.4395,  0.1475,  ...,  0.4824,  0.8906,  0.3477],\n",
      "         [ 1.6406, -0.3398,  0.8906,  ...,  0.6641,  0.2002,  0.2266],\n",
      "         ...,\n",
      "         [-0.8945,  1.2031, -0.0410,  ..., -0.5000,  0.1040,  1.3750],\n",
      "         [ 0.2656,  0.2451,  0.8125,  ...,  0.0449,  0.7852,  0.3945],\n",
      "         [-0.4004,  0.0486, -0.1406,  ...,  1.2266,  1.7656,  1.0312]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    with probe:\n",
    "        out = model(input_ids, position_ids=position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec02651d-1276-4926-8cc9-ead6b44a3bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe2 = Probe('llama-debug/orig/log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "703993ce-2455-4b4a-b4b9-14f877df9143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0356,  0.0009, -0.0229,  ..., -0.0317,  0.0359, -0.0065],\n",
      "         [ 0.0009,  0.0068, -0.0610,  ..., -0.0201,  0.0206,  0.0133],\n",
      "         [ 0.0045,  0.0459, -0.0669,  ...,  0.0486,  0.0190, -0.0203],\n",
      "         ...,\n",
      "         [ 0.0278,  0.0013, -0.0620,  ...,  0.0254,  0.0493,  0.0156],\n",
      "         [-0.0014, -0.0036, -0.0376,  ...,  0.0032,  0.0173,  0.0238],\n",
      "         [-0.0283,  0.0019, -0.0581,  ..., -0.0183, -0.0032,  0.0242]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[ 0.0118,  0.0088, -0.1660,  ..., -0.0583, -0.0361, -0.0391],\n",
      "         [ 0.0140, -0.0047, -0.1533,  ..., -0.0889, -0.0232, -0.0126],\n",
      "         [ 0.0315, -0.0065, -0.1660,  ...,  0.0160, -0.0283, -0.0781],\n",
      "         ...,\n",
      "         [ 0.0144, -0.0742, -0.1426,  ..., -0.0057,  0.0354,  0.0454],\n",
      "         [ 0.0068, -0.0430, -0.0654,  ..., -0.0554, -0.0073,  0.0330],\n",
      "         [-0.0312, -0.0297, -0.0264,  ..., -0.0015, -0.0371,  0.0188]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.0532,  0.2539,  2.1250,  ...,  2.5000,  0.2773,  0.2773],\n",
      "         [ 0.0298, -0.0177, -0.2129,  ...,  0.0081,  0.0184, -0.0287],\n",
      "         [ 0.0752, -0.0195, -0.1787,  ...,  0.0586, -0.0093, -0.0645],\n",
      "         ...,\n",
      "         [ 0.0938, -0.1162, -0.1904,  ...,  0.0251,  0.0762,  0.1260],\n",
      "         [ 0.0193, -0.0815, -0.0544,  ...,  0.0293,  0.0654,  0.0208],\n",
      "         [-0.0080, -0.0094, -0.0481,  ..., -0.0112, -0.0171,  0.0100]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.0583,  0.2949,  2.1719,  ...,  2.5469,  0.3223,  0.2539],\n",
      "         [ 0.0190, -0.0530, -0.3223,  ...,  0.0513, -0.0386, -0.0938],\n",
      "         [ 0.0581, -0.0791, -0.2578,  ...,  0.0332, -0.0864, -0.0684],\n",
      "         ...,\n",
      "         [ 0.1157, -0.1465, -0.3125,  ...,  0.0498,  0.0933, -0.0112],\n",
      "         [ 0.0889, -0.1108, -0.0791,  ...,  0.0967,  0.1045, -0.0148],\n",
      "         [-0.0518,  0.0282, -0.1035,  ...,  0.0435, -0.0540,  0.0232]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-4.6631e-02,  3.5352e-01,  2.2031e+00,  ...,  2.5625e+00,\n",
      "           2.6562e-01,  2.6562e-01],\n",
      "         [ 3.1494e-02, -1.5918e-01, -3.1836e-01,  ...,  6.9336e-02,\n",
      "          -1.1621e-01, -1.2012e-01],\n",
      "         [ 1.7090e-02, -1.3477e-01, -3.1250e-01,  ...,  6.2988e-02,\n",
      "          -1.7285e-01, -3.6621e-03],\n",
      "         ...,\n",
      "         [ 9.0820e-02, -9.1797e-02, -3.2617e-01,  ...,  6.7871e-02,\n",
      "          -3.7109e-02,  8.9111e-03],\n",
      "         [ 8.1543e-02, -1.0645e-01, -6.8359e-02,  ...,  7.5684e-02,\n",
      "           1.0498e-02,  4.2480e-02],\n",
      "         [-6.0791e-02, -1.3184e-02, -1.4062e-01,  ..., -1.2207e-03,\n",
      "          -1.1230e-02,  1.2207e-01]]], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.0742,  0.3906,  2.2812,  ...,  2.5312,  0.3320,  0.3223],\n",
      "         [ 0.0186, -0.0244, -0.3086,  ...,  0.0986, -0.0811, -0.0693],\n",
      "         [ 0.0752, -0.1426, -0.3164,  ...,  0.0303, -0.0830, -0.0082],\n",
      "         ...,\n",
      "         [ 0.1426, -0.1973, -0.2773,  ..., -0.1846, -0.1455, -0.0427],\n",
      "         [ 0.0349, -0.2080,  0.0129,  ..., -0.0317, -0.1138,  0.0164],\n",
      "         [-0.0231, -0.0728, -0.1328,  ..., -0.0664, -0.0981,  0.0286]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-8.8867e-02,  4.8047e-01,  2.3125e+00,  ...,  2.5938e+00,\n",
      "           3.3008e-01,  2.8711e-01],\n",
      "         [-4.9316e-02,  2.3560e-02, -2.8906e-01,  ...,  1.0596e-01,\n",
      "           1.7090e-02, -9.5215e-03],\n",
      "         [ 1.0254e-01, -1.7212e-02, -2.5586e-01,  ...,  5.0537e-02,\n",
      "          -6.9824e-02,  3.5645e-02],\n",
      "         ...,\n",
      "         [ 8.0566e-03, -1.3867e-01, -1.7871e-01,  ..., -7.9102e-02,\n",
      "          -1.7676e-01,  4.9561e-02],\n",
      "         [-2.9541e-02, -1.2354e-01,  9.1797e-02,  ..., -2.6367e-02,\n",
      "          -1.9043e-01,  7.9102e-02],\n",
      "         [-9.0820e-02, -4.6631e-02, -1.8799e-02,  ...,  2.0020e-02,\n",
      "          -1.6797e-01,  1.4648e-03]]], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.0688,  0.5352,  2.3281,  ...,  2.5000,  0.3184,  0.3516],\n",
      "         [-0.1182,  0.0571, -0.2578,  ...,  0.1348,  0.0967,  0.0052],\n",
      "         [ 0.0352, -0.0127, -0.2471,  ...,  0.1230, -0.0513,  0.0115],\n",
      "         ...,\n",
      "         [-0.0645,  0.0635, -0.1992,  ..., -0.0128,  0.0942, -0.0120],\n",
      "         [-0.1328,  0.0151,  0.0684,  ..., -0.0396, -0.0144, -0.0132],\n",
      "         [-0.0742,  0.1523,  0.0171,  ...,  0.0040, -0.1006, -0.0732]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.1250,  0.6094,  2.2969,  ...,  2.3750,  0.2812,  0.3516],\n",
      "         [-0.0874,  0.0308, -0.2168,  ..., -0.0549,  0.0461, -0.0214],\n",
      "         [ 0.0371, -0.1309, -0.1924,  ...,  0.1309, -0.0332, -0.0266],\n",
      "         ...,\n",
      "         [ 0.0374, -0.1191, -0.2061,  ...,  0.0299,  0.0752, -0.0056],\n",
      "         [-0.0298, -0.0688,  0.0640,  ...,  0.0115,  0.0342, -0.0830],\n",
      "         [ 0.0464,  0.1377,  0.0071,  ..., -0.0024, -0.0674, -0.0297]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-1.4258e-01,  6.0938e-01,  2.2812e+00,  ...,  2.3281e+00,\n",
      "           3.0078e-01,  4.0820e-01],\n",
      "         [-2.6367e-02,  1.0254e-02, -2.0215e-01,  ..., -1.3672e-01,\n",
      "          -9.7656e-04, -2.3804e-02],\n",
      "         [ 7.9102e-02, -9.7656e-02, -1.7773e-01,  ...,  1.0986e-01,\n",
      "          -5.1025e-02, -8.8867e-02],\n",
      "         ...,\n",
      "         [-1.0449e-01, -1.6211e-01, -1.5723e-01,  ...,  7.6172e-02,\n",
      "           9.4727e-02, -1.0938e-01],\n",
      "         [-2.0117e-01, -1.2256e-01,  8.4473e-02,  ...,  2.0752e-02,\n",
      "           1.8188e-02, -9.7168e-02],\n",
      "         [-1.1963e-02,  6.8359e-02,  2.1729e-02,  ...,  1.8311e-02,\n",
      "          -3.8574e-02, -6.3477e-02]]], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.0610,  0.6289,  2.2656,  ...,  2.2344,  0.3398,  0.3633],\n",
      "         [ 0.0168,  0.0437, -0.1934,  ..., -0.1011,  0.0151, -0.0884],\n",
      "         [ 0.1777, -0.1260, -0.1689,  ...,  0.1484, -0.0579, -0.0850],\n",
      "         ...,\n",
      "         [-0.0605, -0.1670, -0.1187,  ...,  0.2354,  0.2168, -0.0493],\n",
      "         [-0.2256, -0.2188,  0.0947,  ...,  0.0155, -0.0116,  0.0374],\n",
      "         [-0.0986, -0.0840, -0.0123,  ...,  0.0898, -0.0605,  0.0122]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.0923,  0.6758,  2.2344,  ...,  2.0938,  0.2969,  0.4180],\n",
      "         [-0.1318, -0.0383, -0.1973,  ..., -0.0605,  0.1172,  0.0048],\n",
      "         [ 0.0266, -0.1055, -0.2129,  ...,  0.0767, -0.0298, -0.0674],\n",
      "         ...,\n",
      "         [-0.1191, -0.1699, -0.1104,  ...,  0.1699,  0.1069, -0.0415],\n",
      "         [-0.2500, -0.2109,  0.0908,  ..., -0.0527,  0.0154,  0.1406],\n",
      "         [-0.2266, -0.1445, -0.0165,  ..., -0.0195, -0.0688,  0.0269]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.1562,  0.7188,  2.1562,  ...,  1.8594,  0.3320,  0.3730],\n",
      "         [-0.1758, -0.1182, -0.2949,  ..., -0.1035,  0.0991,  0.0327],\n",
      "         [ 0.0649, -0.1934, -0.2773,  ..., -0.0125, -0.0464, -0.0186],\n",
      "         ...,\n",
      "         [-0.0962, -0.2949, -0.1680,  ..., -0.0051,  0.0059, -0.0918],\n",
      "         [-0.1973, -0.3359,  0.0703,  ..., -0.0942, -0.0698,  0.0383],\n",
      "         [-0.1670, -0.3145, -0.1299,  ..., -0.1123, -0.0190,  0.0203]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-1.3867e-01,  6.2500e-01,  2.0156e+00,  ...,  1.6406e+00,\n",
      "           3.1250e-01,  3.7695e-01],\n",
      "         [-9.8145e-02, -1.4746e-01, -3.2617e-01,  ..., -1.7090e-01,\n",
      "           7.2754e-02,  5.8594e-02],\n",
      "         [ 8.2520e-02, -1.6699e-01, -2.2070e-01,  ...,  1.8066e-02,\n",
      "          -4.5166e-02,  6.4453e-02],\n",
      "         ...,\n",
      "         [-9.7168e-02, -2.7930e-01, -1.7285e-01,  ...,  1.8555e-02,\n",
      "          -4.9316e-02, -8.9355e-02],\n",
      "         [-1.8359e-01, -2.5000e-01,  1.7383e-01,  ..., -7.7637e-02,\n",
      "          -7.1777e-02,  4.0283e-03],\n",
      "         [-1.1914e-01, -3.3203e-01, -1.6016e-01,  ..., -8.4961e-02,\n",
      "           1.8311e-04, -9.7046e-03]]], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.0850,  0.4043,  1.6328,  ...,  1.4219,  0.4570,  0.3594],\n",
      "         [ 0.1030, -0.1738, -0.3770,  ..., -0.0938,  0.0608,  0.0222],\n",
      "         [ 0.0503, -0.1206, -0.2539,  ...,  0.0718,  0.0054,  0.0928],\n",
      "         ...,\n",
      "         [-0.0757, -0.2559, -0.2793,  ...,  0.0176, -0.0112, -0.1338],\n",
      "         [-0.1377, -0.2129,  0.1865,  ..., -0.0228, -0.0371, -0.0874],\n",
      "         [-0.1191, -0.2012, -0.1836,  ..., -0.0535,  0.0479,  0.0212]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-1.8945e-01,  3.4570e-01,  1.5391e+00,  ...,  1.3047e+00,\n",
      "           5.9766e-01,  3.7109e-01],\n",
      "         [ 2.5977e-01, -2.8516e-01, -2.9688e-01,  ..., -7.7637e-02,\n",
      "           2.4316e-01,  3.5645e-02],\n",
      "         [ 1.3574e-01, -1.0596e-01, -2.2949e-01,  ...,  1.4258e-01,\n",
      "          -5.4688e-02, -2.2461e-02],\n",
      "         ...,\n",
      "         [-6.8848e-02, -2.3730e-01, -2.8320e-01,  ...,  1.3672e-02,\n",
      "           4.2236e-02, -1.5918e-01],\n",
      "         [ 4.8828e-04, -2.3633e-01,  2.3535e-01,  ..., -4.1992e-02,\n",
      "          -7.7148e-02, -1.6309e-01],\n",
      "         [ 1.5137e-02, -1.5039e-01, -1.6211e-01,  ...,  1.5137e-02,\n",
      "          -1.1523e-01, -5.2490e-02]]], device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.1895,  0.2676,  1.4844,  ...,  1.2812,  0.5977,  0.3145],\n",
      "         [ 0.1504, -0.4395, -0.4004,  ..., -0.0220,  0.1406,  0.0376],\n",
      "         [ 0.0508, -0.0217, -0.3535,  ...,  0.1133,  0.0576,  0.0420],\n",
      "         ...,\n",
      "         [-0.0610, -0.2598, -0.3750,  ..., -0.0703, -0.0024, -0.2461],\n",
      "         [-0.0276, -0.2168,  0.1992,  ...,  0.0469, -0.0403, -0.2217],\n",
      "         [-0.0986, -0.2451, -0.2637,  ...,  0.0742,  0.0635, -0.2305]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.2051,  0.2080,  1.4062,  ...,  1.2578,  0.5156,  0.3711],\n",
      "         [ 0.3398, -0.2715, -0.4238,  ...,  0.1885,  0.1758,  0.1299],\n",
      "         [ 0.2871, -0.1069, -0.2754,  ...,  0.2051,  0.1680, -0.0977],\n",
      "         ...,\n",
      "         [-0.0579, -0.2139, -0.3438,  ...,  0.0708,  0.2812, -0.0728],\n",
      "         [ 0.0430, -0.2012,  0.2432,  ...,  0.0693,  0.0859, -0.1201],\n",
      "         [-0.0425, -0.2832, -0.1680,  ...,  0.1152,  0.2754, -0.1406]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.2217,  0.1650,  1.3750,  ...,  1.2422,  0.5000,  0.4434],\n",
      "         [ 0.4766, -0.3945, -0.4805,  ...,  0.1660,  0.2119,  0.2148],\n",
      "         [ 0.3926, -0.1709, -0.0479,  ...,  0.0242,  0.1582, -0.0138],\n",
      "         ...,\n",
      "         [ 0.0253, -0.2617, -0.2344,  ...,  0.0791,  0.1445,  0.0176],\n",
      "         [ 0.1973, -0.2910,  0.3203,  ...,  0.2393, -0.1167, -0.0474],\n",
      "         [ 0.0186, -0.1455, -0.1455,  ...,  0.1514, -0.0200, -0.0146]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.2266,  0.0986,  1.2969,  ...,  1.2500,  0.4648,  0.4766],\n",
      "         [ 0.5781, -0.3340, -0.5547,  ...,  0.1533,  0.1855,  0.3848],\n",
      "         [ 0.5625, -0.1582, -0.1680,  ...,  0.0605,  0.1719,  0.0576],\n",
      "         ...,\n",
      "         [ 0.0031, -0.3164, -0.4102,  ...,  0.1895,  0.4102,  0.0981],\n",
      "         [ 0.3633, -0.3652,  0.2451,  ...,  0.2461,  0.0344, -0.1377],\n",
      "         [ 0.2070, -0.1670, -0.2754,  ...,  0.1807,  0.2246, -0.2021]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.2207,  0.0219,  1.2734,  ...,  1.2188,  0.4707,  0.5078],\n",
      "         [ 0.5898, -0.4863, -0.4766,  ...,  0.2910,  0.2461,  0.4902],\n",
      "         [ 0.6250, -0.1445, -0.1631,  ..., -0.0374,  0.0674,  0.0608],\n",
      "         ...,\n",
      "         [-0.0493, -0.2080, -0.3223,  ...,  0.1162,  0.3223,  0.3145],\n",
      "         [ 0.1602, -0.3926,  0.5000,  ...,  0.2471, -0.0928,  0.1123],\n",
      "         [ 0.0928, -0.1992, -0.0566,  ...,  0.1670,  0.1650,  0.1543]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.1846, -0.0967,  1.2188,  ...,  1.2031,  0.4414,  0.5039],\n",
      "         [ 0.5234, -0.4375, -0.3887,  ...,  0.1455,  0.1875,  0.5078],\n",
      "         [ 0.6641, -0.0413, -0.0889,  ...,  0.1953, -0.0188,  0.0312],\n",
      "         ...,\n",
      "         [ 0.0464, -0.1914, -0.3047,  ...,  0.1777,  0.4336,  0.3750],\n",
      "         [ 0.2373, -0.2559,  0.5781,  ...,  0.2236, -0.0103,  0.1797],\n",
      "         [ 0.1934, -0.1123, -0.0947,  ...,  0.1797,  0.3711,  0.1021]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.1914, -0.2051,  1.1172,  ...,  1.1875,  0.4043,  0.5625],\n",
      "         [ 0.5078, -0.2383, -0.6133,  ...,  0.1680,  0.4297,  0.4805],\n",
      "         [ 0.6836, -0.0083, -0.2070,  ...,  0.1777,  0.2188,  0.0806],\n",
      "         ...,\n",
      "         [-0.0554, -0.2852, -0.3594,  ...,  0.0227,  0.4922,  0.4863],\n",
      "         [ 0.2832, -0.1875,  0.6484,  ...,  0.1133,  0.0850,  0.2988],\n",
      "         [ 0.0107, -0.2090, -0.1602,  ..., -0.0014,  0.4023,  0.1230]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.1006, -0.3203,  1.1094,  ...,  1.1328,  0.3633,  0.6172],\n",
      "         [ 0.5156, -0.2363, -0.6719,  ...,  0.2500,  0.3438,  0.7031],\n",
      "         [ 0.5859, -0.0420, -0.2158,  ...,  0.2617,  0.0918,  0.1157],\n",
      "         ...,\n",
      "         [-0.0840,  0.3223, -0.3086,  ..., -0.0029,  0.6250,  0.9219],\n",
      "         [ 0.4258, -0.0137,  0.8164,  ..., -0.1328,  0.3438,  0.6172],\n",
      "         [ 0.1641,  0.0435, -0.2539,  ..., -0.0322,  0.5898,  0.7305]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.0967, -0.4082,  1.0859,  ...,  1.0391,  0.3340,  0.6719],\n",
      "         [ 0.2812, -0.1895, -0.5469,  ...,  0.3379,  0.5586,  0.7891],\n",
      "         [ 0.3711,  0.0625, -0.1221,  ...,  0.5820,  0.2090,  0.1377],\n",
      "         ...,\n",
      "         [-0.1221,  0.5312, -0.3164,  ...,  0.0723,  0.6836,  0.9727],\n",
      "         [ 0.5469,  0.0452,  0.7188,  ...,  0.0469,  0.6836,  0.7734],\n",
      "         [ 0.1885,  0.1406, -0.4023,  ...,  0.0337,  0.6172,  0.8672]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[ 0.0222, -0.6094,  0.8945,  ...,  0.9297,  0.2832,  0.8906],\n",
      "         [ 0.5430, -0.3340, -0.3438,  ...,  0.3691,  0.4375,  0.7344],\n",
      "         [ 0.6719,  0.0476, -0.0186,  ...,  0.6367,  0.0332,  0.2051],\n",
      "         ...,\n",
      "         [-0.3438,  0.7656, -0.5625,  ...,  0.1064,  0.5586,  0.9141],\n",
      "         [ 0.7578,  0.0864,  0.5547,  ...,  0.4297,  0.5234,  1.2031],\n",
      "         [-0.0649, -0.2451, -0.5781,  ...,  0.0366,  0.7930,  1.5000]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-0.6367, -1.0859,  1.1875,  ...,  1.9844, -0.0270,  1.3594],\n",
      "         [ 0.8086, -0.3516, -0.0884,  ...,  0.3320,  0.6328,  0.2871],\n",
      "         [ 0.9844, -0.0437,  0.3711,  ...,  0.3047,  0.0542,  0.2969],\n",
      "         ...,\n",
      "         [-0.5664,  0.7188, -0.2559,  ..., -0.3418,  0.4707,  1.3516],\n",
      "         [ 0.3145, -0.2402,  0.5391,  ...,  0.3691,  0.8359,  1.0859],\n",
      "         [ 0.0334, -0.2871, -0.3691,  ...,  0.1367,  0.6523,  1.4922]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n",
      "tensor([[[-1.0156, -0.9180,  1.1406,  ...,  2.8281, -0.5391,  0.5703],\n",
      "         [ 1.0625, -0.4395,  0.1475,  ...,  0.4824,  0.8906,  0.3477],\n",
      "         [ 1.6406, -0.3398,  0.8906,  ...,  0.6641,  0.2002,  0.2266],\n",
      "         ...,\n",
      "         [-0.8984,  1.1875, -0.0449,  ..., -0.4980,  0.1157,  1.3750],\n",
      "         [ 0.2734,  0.2441,  0.8125,  ...,  0.0454,  0.7891,  0.3867],\n",
      "         [-0.4043,  0.0601, -0.1406,  ...,  1.2109,  1.7969,  1.0234]]],\n",
      "       device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    with probe2:\n",
    "        out_orig = model_orig(input_ids, start_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "227e94b1-56e3-4f68-bf1b-50c8801a06ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_orig = 'Transformer.layers.20.feed_forward.w2__out'\n",
    "key = 'TransformerDecoder.blocks.20.ff.module.linear_out__out'\n",
    "wo_in_orig = torch.load(f'llama-debug/orig/tensors/v4/{key_orig}.pt', weights_only=True)\n",
    "wo_in = torch.load(f'llama-debug/tensors/v0/{key}.pt', weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2a3e5863-e97a-4481-8a7b-10ff2c6d8fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0059, device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(wo_in - wo_in_orig).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "857c10e4-fa31-4ede-a0fa-166173997519",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0171,  0.0269, -0.0172,  ..., -0.0222, -0.0461,  0.0041],\n",
       "        [ 0.0166, -0.0889, -0.0042,  ..., -0.0583, -0.0884, -0.0298],\n",
       "        [-0.0306, -0.0165, -0.0583,  ..., -0.0435, -0.0840,  0.0106],\n",
       "        ...,\n",
       "        [-0.0286,  0.0099, -0.0488,  ..., -0.0581, -0.0942, -0.0155],\n",
       "        [-0.0047, -0.0471, -0.0386,  ..., -0.0664, -0.0781,  0.0035],\n",
       "        [-0.0261, -0.0908, -0.0952,  ..., -0.0344,  0.0737,  0.0625]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wo_in_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "273c6cf7-ade4-4b6e-8cd3-2e3f6a1835c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0171,  0.0269, -0.0172,  ..., -0.0222, -0.0461,  0.0041],\n",
       "        [ 0.0166, -0.0889, -0.0042,  ..., -0.0583, -0.0884, -0.0298],\n",
       "        [-0.0306, -0.0165, -0.0583,  ..., -0.0435, -0.0840,  0.0106],\n",
       "        ...,\n",
       "        [-0.0286,  0.0099, -0.0488,  ..., -0.0581, -0.0942, -0.0155],\n",
       "        [-0.0047, -0.0471, -0.0386,  ..., -0.0664, -0.0781,  0.0035],\n",
       "        [-0.0261, -0.0908, -0.0952,  ..., -0.0344,  0.0737,  0.0625]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wo_in_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91a83f47-dcaf-43dc-ab66-7c7c228b2b11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.5469,  3.9219,  3.9688,  ..., -4.6875, -4.6875, -4.6875],\n",
       "         [ 6.5625,  7.0000,  4.7188,  ..., -4.0000, -4.0000, -4.0000],\n",
       "         [ 9.3125,  8.3125, 11.0625,  ..., -4.2188, -4.2188, -4.2188],\n",
       "         ...,\n",
       "         [10.1250,  1.7266,  7.8438,  ..., -2.9688, -2.9688, -2.9688],\n",
       "         [ 3.5781,  0.5000,  7.2188,  ..., -3.5781, -3.5781, -3.5781],\n",
       "         [ 1.6875, -2.6406,  3.9375,  ..., -2.4219, -2.4219, -2.4219]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "20a189e5-cbeb-4b96-ac62-8b1f8cbb5dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.5469,  3.9219,  3.9688,  ..., -4.6875, -4.6875, -4.6875],\n",
       "         [ 6.5938,  7.0312,  4.7188,  ..., -4.0000, -4.0000, -4.0000],\n",
       "         [ 9.3750,  8.3750, 11.0625,  ..., -4.2188, -4.2188, -4.2188],\n",
       "         ...,\n",
       "         [10.1250,  1.7109,  7.8750,  ..., -2.9844, -2.9844, -2.9844],\n",
       "         [ 3.5781,  0.5781,  7.2500,  ..., -3.6250, -3.6250, -3.6250],\n",
       "         [ 1.6875, -2.6094,  3.9375,  ..., -2.4531, -2.4531, -2.4531]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "37888c86-6395-4f91-beb6-432304b24f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.5469,  3.9219,  3.9688,  ..., -4.6875, -4.6875, -4.6875],\n",
       "         [ 6.5625,  7.0000,  4.7188,  ..., -4.0000, -4.0000, -4.0000],\n",
       "         [ 9.3125,  8.3125, 11.0625,  ..., -4.2188, -4.2188, -4.2188],\n",
       "         ...,\n",
       "         [10.1250,  1.7266,  7.8438,  ..., -2.9688, -2.9688, -2.9688],\n",
       "         [ 3.5781,  0.5000,  7.2188,  ..., -3.5781, -3.5781, -3.5781],\n",
       "         [ 1.7188, -2.6562,  3.9375,  ..., -2.4062, -2.4062, -2.4062]]],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70090038-e66e-4420-90a5-ce437aeb90f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2188, device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out.logits - out_orig).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d86d51e0-0255-484c-8000-6438565cf009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0938, device='cuda:0')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(out.logits - out_orig).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1f4229ee-d51a-4ca1-801c-2d857ef8627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm /mnt/naplm/seonghyeon/model.pt /mnt/naplm/seonghyeon/model_orig.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7861258-acf5-47a5-b33a-3c48882395c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_42590/4248749713.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  qkv = torch.load('/mnt/naplm/seonghyeon/model.pt')\n",
      "/tmp/ipykernel_42590/4248749713.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  qkv_orig = torch.load('/mnt/naplm/seonghyeon/model_orig.pt')\n"
     ]
    }
   ],
   "source": [
    "qkv = torch.load('/mnt/naplm/seonghyeon/model.pt')\n",
    "qkv_orig = torch.load('/mnt/naplm/seonghyeon/model_orig.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4768075f-3fb1-47de-947c-d52edd68bf29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(qkv[0] - qkv_orig[0]).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "edcc1376-fa81-480a-803e-4ce254fc9dcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(qkv[1] - qkv_orig[1]).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5d4f8b34-8346-47d9-af7b-1ee13cbb9b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(qkv[3].float() - qkv_orig[3].float()).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "635c00ef-ae12-44be-9874-efc55fb75d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 24, 128])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv_orig[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "01c8f62c-dead-4f35-8443-ae1f994a4f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flash_attn import flash_attn_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "300a1ed8-69c0-4d80-96c6-219f67a82b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 24, 128]), torch.Size([1, 8, 24, 128]))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv[0].shape, qkv_orig[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9ec9db63-7efb-4722-8b3d-63b063406dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0312, device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(qkv[1].reshape(1, 8, 8, 2, 64).transpose(3, 4).reshape(1, 8, 8, 128) - qkv_orig[1]).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "278e141e-96e8-414c-9233-905c7166c72f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3652, -0.3867, -0.5703,  0.7031,  0.4648,  0.4414,  0.6367,  0.4336,\n",
       "        -0.2734,  0.6367,  0.0339, -1.0859,  1.7500, -0.8594,  0.1079, -1.2188,\n",
       "        -0.4395,  2.4062,  0.3301,  0.2559,  0.0991, -2.0625,  0.6836, -0.6289,\n",
       "         0.4043,  0.2852, -1.0234, -0.2949,  1.5312,  1.2422, -1.4531,  0.0767,\n",
       "        -0.4473, -0.6406, -1.9609,  0.9297,  1.0859,  1.3906,  1.0078,  0.6641,\n",
       "         0.7266, -0.2256,  1.9531, -2.0781,  0.3379,  0.8984, -2.0781,  2.0469,\n",
       "         0.0286,  0.8281,  0.5312,  0.9648, -1.5781, -3.1250,  0.4980, -0.6055,\n",
       "         0.6016,  0.7188,  2.2344,  0.6992, -1.0938, -0.5469, -0.4766, -1.6016,\n",
       "         1.5625,  0.6680, -2.1094,  0.4961, -0.0330,  2.0469, -0.3105, -1.4297,\n",
       "        -0.0222, -1.7188, -1.5234, -0.6445,  0.1377,  5.1250,  0.1904, -4.0000,\n",
       "         0.5352, -2.2500,  1.7969,  0.0894, -0.3340,  1.0078, -0.7148, -2.2812,\n",
       "        -1.6250, -1.4922, -0.6758,  2.0469,  0.5898, -2.4688, -0.0952,  2.6094,\n",
       "         1.8672, -0.6016,  0.8594,  2.0156, -1.4062,  1.1328, -0.4062, -1.9062,\n",
       "        -1.7500,  0.6992, -1.5391, -0.7305,  1.6875, -0.1553,  1.8750, -0.6094,\n",
       "         1.7422, -1.3750,  1.5703, -1.7891,  1.4922,  0.7891,  0.4043,  1.2031,\n",
       "         0.8086, -0.8750, -0.6016, -0.3828,  0.4414,  1.5312, -1.4844, -1.4609],\n",
       "       device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv_orig[0][0, 0, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "52a5cfce-7e19-4270-a319-d4171b7154e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.bfloat16, grad_fn=<MaxBackward1>)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(model.blocks['0'].self_attention.module.qkv.weight[:3072].reshape(24, 2, 64, -1).transpose(1, 2).reshape(3072, 3072) - model_orig.layers[0].attention.wq.weight).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "f2e52bc5-c0b0-4f69-9080-9faea478b842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5120, 3072])"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.blocks['0'].self_attention.module.qkv.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "046947c6-e93e-48bd-b792-f72605480439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(qkv[0] - qkv_orig[0]).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a2625d9-c4d3-4ae8-bf86-709359d83da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23.6250, device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(qkv[1] - qkv_orig[1]).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9fa1339b-dd8c-4e27-807f-3a55be2117c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.bfloat16),\n",
       "indices=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(qkv[1].reshape(1, 8, 24, 2, 64).transpose(3, 4).reshape(1, 8, 3072) - qkv_orig[1]).reshape(-1, 128).abs().max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46cac14-87f9-42d1-a787-637c66b2962e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(qkv[1].reshape(1, 8, 24, 2, 64).transpose(3, 4).reshape(1, 8, 3072) - qkv_orig[1]).reshape(-1, 128).abs().max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "aacd572e-843e-4003-849f-edc912b6dec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 3072]), torch.Size([1, 8, 3072]))"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv[1].shape, qkv_orig[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "24c606a4-4b06-4428-aa31-09e8c67b25cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8 * 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "b0f1648e-35e9-42dd-b75a-87710aac92e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24576"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "8 * 24 * 2 * 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2e200ae1-add3-409c-a856-008eb2263242",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(18.3750, device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(qkv[2] - qkv_orig[2]).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c3aec29c-7a89-4ac9-83ba-26981806defd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0', dtype=torch.bfloat16),\n",
       "indices=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0'))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(qkv[2].reshape(1, 8, 8, 2, 64).transpose(3, 4).reshape(1, 8, 1024) - qkv_orig[2]).reshape(-1, 128).abs().max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "94cc575d-1214-48a9-a65f-6130a940127e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 8, 1024]), torch.Size([1, 8, 1024]))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qkv[2].shape, qkv_orig[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a3a9f9cb-dd83-47ce-b4a5-2f0cbb7305a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = qkv[1].reshape(1, 8, 24, 128)\n",
    "k1 = qkv[2].reshape(1, 8, 8, 128)\n",
    "v1 = qkv[3].reshape(1, 8, 8, 128)\n",
    "q2 = qkv_orig[1].reshape(1, 8, 24, 128)\n",
    "k2 = qkv_orig[2].reshape(1, 8, 8, 128)\n",
    "v2 = qkv_orig[3].reshape(1, 8, 8, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "43f78608-b105-487d-818c-dc64b6118ec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:0', dtype=torch.bfloat16)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(flash_attn_func(q1, k1, v1, causal=True) - flash_attn_func(q2, k2, v2, causal=True)).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "384f2086-b615-48a4-b4cf-5eae2180f04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(2 * 4).reshape(2, 1, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a1d22f8-94bc-40ef-bcad-fca99fefa34d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoder(\n",
       "  (embedding): TextEmbedding(\n",
       "    (embed_input_ids): Embedding(128256, 3072)\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (pos_embed): Llama3RoPE()\n",
       "  (blocks): ModuleDict(\n",
       "    (0): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (1): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (2): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (3): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (4): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (5): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (6): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (7): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (8): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (9): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (10): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (11): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (12): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (13): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (14): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (15): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (16): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (17): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (18): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (19): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (20): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (21): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (22): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (23): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (24): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (25): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (26): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (27): TransformerEncoderBlock(\n",
       "      (self_attention): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): SelfAttention(\n",
       "          (qkv): Linear(in_features=3072, out_features=5120, bias=False)\n",
       "          (attention): Attention(\n",
       "            (attn_drop): Dropout(p=0, inplace=False)\n",
       "          )\n",
       "          (out): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (ff): ResidualBlock(\n",
       "        (pre_norm): RMSNorm()\n",
       "        (module): FeedForward(\n",
       "          (linear1): Linear(in_features=3072, out_features=16384, bias=False)\n",
       "          (activation): SwiGLU()\n",
       "          (dropout): Dropout(p=0, inplace=False)\n",
       "          (linear2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        )\n",
       "        (dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (post_blocks): SequenceParallelWrapper(\n",
       "    (module): RMSNorm()\n",
       "  )\n",
       "  (head): VocabParallelLinear(\n",
       "    (linear): Linear(in_features=3072, out_features=128256, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "899435be-be19-41b5-a4ed-39c028c8cd5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'int' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;241;43m5\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'int' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "5 * None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1ca3b740-5238-42b2-b12f-37e1ba55c6d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9216 / 3072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb171acd-d9e2-4216-8052-c8185bc791fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (tok_embeddings): Embedding(128256, 3072)\n",
       "  (layers): ModuleList(\n",
       "    (0-27): 28 x TransformerBlock(\n",
       "      (attention): Attention(\n",
       "        (wq): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "        (wk): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "        (wv): Linear(in_features=3072, out_features=1024, bias=False)\n",
       "        (wo): Linear(in_features=3072, out_features=3072, bias=False)\n",
       "      )\n",
       "      (feed_forward): FeedForward(\n",
       "        (w1): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "        (w2): Linear(in_features=8192, out_features=3072, bias=False)\n",
       "        (w3): Linear(in_features=3072, out_features=8192, bias=False)\n",
       "      )\n",
       "      (attention_norm): RMSNorm()\n",
       "      (ffn_norm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (norm): RMSNorm()\n",
       "  (output): Linear(in_features=3072, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc7429ad-1b61-47de-8742-24edf1420bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5120"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3072 + 1024 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d98f51f-3fee-49ea-bc4c-3cd5e5c91753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
